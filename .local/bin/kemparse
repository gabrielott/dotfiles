#!/usr/bin/env python3

# Parses the text content of all posts from a creator in kemono.party
# Use the flag --help to see a list of options.

import requests, argparse
from bs4 import BeautifulSoup
from subprocess import Popen, PIPE
from urllib.parse import urljoin, urlparse

def get_cookies():
    p = Popen(["firecookies", "kemono.party"], stdout=PIPE)
    out, _ = p.communicate()
    out = out.decode("utf-8")

    cookies = {}
    for line in out.splitlines()[1:]:
        split = line.split("\t")
        cookies[split[5]] = split[6]

    return cookies

def parse_post(s, url):
    r = s.get(url)
    soup = BeautifulSoup(r.text, "html.parser")

    title = soup.find(class_="post__title").text.strip()
    timestamp = soup.find(class_="timestamp").text.strip()
    print(f"=========== {timestamp} / {title}")

    if content := soup.find(class_="post__content"):
        print(content.text.strip())
        if links := content.find_all("a"):
            print("=== Links:")
            for link in links:
                print(f"{link.text.strip()} --- {link.get('href').strip()}")

def parse_page(s, url):
    r = s.get(url)
    if "Nobody here but us chickens!" in r.text:
        return False

    soup = BeautifulSoup(r.text, "html.parser")

    for post in soup.find_all(class_="post-card__link"):
        parse_post(s, "https://kemono.party" + post.a["href"])

    return True

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("url", help="url of the creator page to be parsed")
    args = parser.parse_args()
    url = urljoin(args.url, urlparse(args.url).path)

    s = requests.session()
    cookies = get_cookies()
    requests.utils.add_dict_to_cookiejar(s.cookies, cookies)

    offset = 0
    while parse_page(s, f"{url}?o={offset}"):
        offset += 25

if __name__ == "__main__":
    main()
