#!/usr/bin/env python3

# Downloads all posts from a creator in yiff.party
# Use the flag --help to see a list of options.

import sys, os, urllib, argparse
import asyncio, aiohttp
from bs4 import BeautifulSoup

args = None
file_locks = {}

def mkdir_if_not_exists(path):
    try:
        os.mkdir(path)
    except FileExistsError:
        pass

async def download_page(session, url, path, semaphore):
    async with semaphore, session.get(url) as resp:
        print(f"Downloading page {url}")
        soup = BeautifulSoup(await resp.read(), "html.parser")
        posts = soup.find(class_="row yp-posts-row").find_all("div", recursive=False)
    await asyncio.gather(*(download_post(session, post, path, semaphore) for post in posts))

async def download_post(session, post, path, semaphore):
    title = post.find(class_="card-title").text.strip()[:-9]
    body = post.find(class_="post-body").text
    post_time = post.find(class_="post-time").text.split("T")[0]
    dir_path = os.path.join(path, f"{post_time}_{title.replace('/', '-')}")
    files = {}

    # Skip if dir already exists and --continue flag is set
    if args.no_redownload and os.path.exists(dir_path):
        print(f"Skipping post {title} (directory already exists and --continue flag set)")
        return

    mkdir_if_not_exists(dir_path)

    # Media & Attachments
    if not args.no_media:
        media_attachments = post.find_all(class_="card-attachments")
        for content in media_attachments:
            for link_filename in content.find_all("a"):
                files[link_filename["href"]] = link_filename.text

    # Post File
    if not args.no_post_file:
        if card_action := post.find(class_="card-action"):
            if post_file := card_action.find("a", text="Post file"):
                url = post_file["href"]
                filename = os.path.basename(urllib.parse.urlparse(url).path)
                extension = os.path.splitext(filename)[1]
                files[url] = f"post_file{extension}"

    # Embed Data
    if not args.no_embed:
        if embed_data := post.find(class_="card-embed"):
            link = embed_data.find(text="URL").parent.parent.a["href"]
            subject = embed_data.find(text="Subject").parent.parent.find(text=True, recursive=False)
            with open(os.path.join(dir_path, "embed.txt"), "wt") as file:
                file.write(f"{subject}\n{link}\n")

    # Links
    if not args.no_links:
        if links := [a["href"] for a in post.find(class_="post-body").find_all("a")]:
            with open(os.path.join(dir_path, "links.txt"), "wt") as file:
                file.write('\n'.join(links) + "\n")

    # Text
    if not args.no_text_body:
        with open(os.path.join(dir_path, "body.txt"), "wt") as file:
            file.write(body)

    if files:
        tasks = []
        for url, filename in files.items():
            file_path = os.path.join(dir_path, filename)
            tasks.append(download_file(session, url, file_path, semaphore))

        await asyncio.gather(*tasks)

async def download_file(session, url, path, semaphore):
    async with semaphore:
        while True:
            async with session.get(url) as resp:
                if resp.status != 200:
                    print(f"Error while downloading {url}, will retry in a second (status != 200)")
                    await asyncio.sleep(1)
                    continue

                if not file_locks.get(path):
                    file_locks[path] = asyncio.Lock()

                # Ensures there's only one task downloading to a certain path at the same time
                async with file_locks[path]:
                    if os.path.isfile(path):
                        if os.path.getsize(path) == int(resp.headers["Content-Length"]):
                            print(f"{url} seems to contain a duplicate file, skipping")
                            break
                        else:
                            while True:
                                filename, ext = os.path.splitext(path)
                                path = f"{filename}_{ext}"
                                if not os.path.isfile(path):
                                    break

                    print(f"Downloading {url}")
                    with open(path, "wb") as file:
                        try:
                            while chunk := await resp.content.read(1000):
                                file.write(chunk)
                        except aiohttp.ClientPayloadError:
                            print(f"Error while downloading {url}, will retry in a second (ClientPayloadError)")
                            await asyncio.sleep(1)
                            continue

                    # Checks whether what we just downloaded isn't actually a Cloudfare error page
                    with open(path, "rb") as file:
                        if file.read(15).decode(errors="ignore") == "<!DOCTYPE html>":
                            print(f"=== Cloudfare {url} ===")
                            await asyncio.sleep(1)
                            continue
                        print(f"Finished {url}")
                        break

async def main():
    global args

    parser = argparse.ArgumentParser()
    parser.add_argument("url", help="url of the creator page to be downloaded")
    parser.add_argument("-c", "--continue", action="store_true", dest="no_redownload",
                        help="don't download posts that have already been downloaded")
    parser.add_argument("-t", "--no-post-file", action="store_true", dest="no_post_file",
                        help="don't download each post's post file")
    parser.add_argument("-m", "--no-media", action="store_true", dest="no_media",
                        help="don't download files in the attachments and media sections")
    parser.add_argument("-b", "--no-text-body", action="store_true", dest="no_text_body",
                        help="don't create a text file with each post's body")
    parser.add_argument("-e", "--no-embed", action="store_true", dest="no_embed",
                        help="don't create a text file with each post's embedded data")
    parser.add_argument("-l", "--no-links", action="store_true", dest="no_links",
                        help="don't create a text file with each post's links")
    parser.add_argument("-d", "--concurrent-downloads", type=int, dest="concurrent_downloads", default=5,
                        help="maximum number of files that will be downloaded at the same time (default is 5)")
    args = parser.parse_args()

    semaphore = asyncio.BoundedSemaphore(args.concurrent_downloads)

    async with aiohttp.ClientSession() as session:
        tasks = []
        async with session.get(args.url) as resp:
            soup = BeautifulSoup(await resp.read(), "html.parser")
            creator = soup.find(class_="yp-info-name").text.strip().replace("/", "-")
            total_pages = int(soup.find(class_="paginate-count").text.split(" / ")[1])
            mkdir_if_not_exists(creator)

            posts = soup.find(class_="row yp-posts-row").find_all("div", recursive=False)
            tasks.extend(download_post(session, post, creator, semaphore) for post in posts)

        for page_number in range(2, total_pages + 1):
            tasks.append(download_page(session, f"{args.url}?p={page_number}", creator, semaphore))

        await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
